{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBPedia and Wikipedia Mining to get Movie Metadata\n",
    "\n",
    "\n",
    "We use SPARQL queries to get dbpedia metadata for movies released in a year. Then the plot summaries are scraped from Wikipedia for each of these movies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib2\n",
    "from bs4 import BeautifulSoup\n",
    "import httplib\n",
    "import time\n",
    "\n",
    "movies_for_year_query = \"\"\"\n",
    "                PREFIX dbpont: <http://dbpedia.org/ontology/>\n",
    "                PREFIX dbpprop: <http://dbpedia.org/property/>\n",
    "                PREFIX dbres: <http://dbpedia.org/resource/>\n",
    "                PREFIX dbc: <http://dbpedia.org/resource/Category:>\n",
    "                PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "                PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "\n",
    "                SELECT DISTINCT ?title \n",
    "                       (group_concat(distinct ?language;separator=\", \") as ?languages) \n",
    "                       (group_concat(distinct ?country;separator=\", \") as ?countries) \n",
    "                       ?released\n",
    "                       ?gross\n",
    "                       ?comment\n",
    "                       ?abstract\n",
    "                       (?movie as ?dbpediaLink)\n",
    "                       ?wikipediaLink\n",
    "                WHERE\n",
    "                {\n",
    "                        ?movie rdf:type dbpont:Film .\n",
    "                        ?movie rdfs:label ?title .\n",
    "                        ?movie rdfs:comment ?comment .\n",
    "                        ?movie dbpont:abstract ?abstract .\n",
    "                        ?movie dcterms:subject dbc:%(year)s_films . \n",
    "                        OPTIONAL { ?movie dbpprop:released ?released }\n",
    "                        OPTIONAL { ?movie dbpprop:country ?country } \n",
    "                        OPTIONAL { ?movie dbpont:gross ?gross }\n",
    "                        OPTIONAL { ?movie dbpprop:language ?language }\n",
    "                        OPTIONAL { ?movie foaf:isPrimaryTopicOf ?wikipediaLink }\n",
    "\n",
    "                        FILTER (lang(?title) = 'en')\n",
    "                        FILTER (lang(?abstract) = 'en')\n",
    "                        FILTER (lang(?comment) = 'en')\n",
    "                }\n",
    "                \"\"\"\n",
    "\n",
    "def sparql_json_to_df(results, year):\n",
    "    movie_dicts = []\n",
    "\n",
    "    for movie in results['results']['bindings']:\n",
    "        title = movie['title']['value']\n",
    "        languages = movie.get('languages', None)\n",
    "        if(languages):\n",
    "            languages = languages['value']\n",
    "        countries = movie.get('countries', None)\n",
    "        if(countries):\n",
    "            countries = countries['value']\n",
    "        released = movie.get('released', None)\n",
    "        if(released):\n",
    "            released = released['value']\n",
    "        gross = movie.get('gross', None)\n",
    "        if(gross):\n",
    "            gross = gross['value']\n",
    "        comment = movie.get('comment', None)\n",
    "        if(comment):\n",
    "            comment = comment['value']\n",
    "        abstract = movie.get('abstract', None)\n",
    "        if(abstract):\n",
    "            abstract = abstract['value']\n",
    "        dbpediaLink = movie.get('dbpediaLink', None)\n",
    "        if(dbpediaLink):\n",
    "            dbpediaLink = dbpediaLink['value']\n",
    "        wikipediaLink = movie.get('wikipediaLink', None)\n",
    "        if(wikipediaLink):\n",
    "            wikipediaLink = wikipediaLink['value']\n",
    "\n",
    "        movie_dicts.append({'year':year,\n",
    "                            'title':title,\n",
    "                            'languages':languages,\n",
    "                            'countries':countries,\n",
    "                            'released':released,\n",
    "                            'gross':gross,\n",
    "                            'comment':comment,\n",
    "                            'abstract':abstract,\n",
    "                            'dbpediaLink':dbpediaLink,\n",
    "                            'wikipediaLink':wikipediaLink})\n",
    "\n",
    "    df = pd.DataFrame(movie_dicts, columns=['year'] + results['head']['vars'])\n",
    "    return(df)\n",
    "\n",
    "def get_dbpedia_data_for_year(year):\n",
    "    sparql = SPARQLWrapper(\"http://dbpedia.org/sparql\")\n",
    "    sparql.setQuery(movies_for_year_query % {'year': str(year)})\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    year_df = sparql_json_to_df(results, year)\n",
    "    # Do some cleanup\n",
    "    strip_url = lambda x: re.sub(\"http://dbpedia.org/resource/\", \"\", x)\n",
    "    year_df.languages = year_df.languages.apply(strip_url)\n",
    "    year_df.languages = year_df.languages.apply(lambda x: re.sub(\"_language\", \"\", x))\n",
    "    year_df.countries = year_df.countries.apply(strip_url)\n",
    "    # Remove (film) or (<year> film) at the end of the title\n",
    "    year_df.title = year_df.title.apply(lambda x: re.sub(r\"[\\ ]*\\((%s)?[\\ ]*film\\)\" % str(year), \"\", x))\n",
    "    return(year_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re, urlparse\n",
    "\n",
    "'''\n",
    " The iri to uri conversion routines are from \n",
    " http://stackoverflow.com/questions/4389572/how-to-fetch-a-non-ascii-url-with-python-urlopen\n",
    "'''\n",
    "def urlEncodeNonAscii(b):\n",
    "    return re.sub('[\\x80-\\xFF]', lambda c: '%%%02x' % ord(c.group(0)), b)\n",
    "\n",
    "def iri2uri(iri):\n",
    "    parts = urlparse.urlparse(iri)\n",
    "    parts_cleaned = [part.encode('idna') \n",
    "                         if i == 1 else urlEncodeNonAscii(part.encode('utf-8'))\n",
    "                         for i, part in enumerate(parts)]\n",
    "    \n",
    "    return urlparse.urlunparse(parts_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_wikipedia_plot(wiki_link, delay_sec=0.5):\n",
    "    time.sleep(delay_sec)\n",
    "    plot = \"\"\n",
    "    \n",
    "    try: \n",
    "        wiki_resp = urllib2.urlopen(iri2uri(wiki_link))\n",
    "\n",
    "        soup = BeautifulSoup(wiki_resp)    \n",
    "        headers = soup.find_all('h2')\n",
    "        for header in headers:\n",
    "            plot_header = header.find(id=[\"Plot\", \"Summary\", \"Synopsis\"])\n",
    "            if(plot_header == None):\n",
    "                plot_header = header.find(id=\"Story\")\n",
    "            if(plot_header):\n",
    "                for section in header.find_next_siblings():\n",
    "                    if(section.name == \"h2\"):\n",
    "                        break\n",
    "                    plot += section.get_text()\n",
    "\n",
    "    except urllib2.HTTPError, e:\n",
    "        print(wiki_link + ': HTTPError = ' + str(e.code))\n",
    "    except urllib2.URLError, e:\n",
    "        print(wiki_link + ': URLError = ' + str(e.reason))\n",
    "    except httplib.HTTPException, e:\n",
    "        print(wiki_link + ': HTTPException')\n",
    "    except UnicodeEncodeError:\n",
    "        print(wiki_link + \": unicode encode error\")\n",
    "    except Exception:\n",
    "        print(wiki_link + ': generic exception')\n",
    "                \n",
    "    return plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the dbpedia metadata and construct a dataframe for each year. Then, go to the wikipedia link and extract the plot summary if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "years = range(2000, 2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://en.wikipedia.org/wiki/Comedy_of_Innocence: HTTPException\n",
      "http://en.wikipedia.org/wiki/Hamlet_(2000_film): URLError = [Errno 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n",
      "http://en.wikipedia.org/wiki/2001:_A_Space_Travesty: URLError = [Errno 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n",
      "http://en.wikipedia.org/wiki/Amy_Stiller's_Breast: HTTPError = 404\n",
      "http://en.wikipedia.org/wiki/Hindustan_the_Mother: HTTPError = 404\n",
      "http://en.wikipedia.org/wiki/Oh_Nanna_Nalle: HTTPError = 404\n",
      "http://en.wikipedia.org/wiki/Faithless_(2000_film): generic exception\n",
      "http://en.wikipedia.org/wiki/Long_Night's_Journey_into_Day: HTTPException\n",
      "http://en.wikipedia.org/wiki/Sradha: generic exception\n",
      "('1171', ' records for the year ', '2000', 'written to ', 'wikipedia_plots_2000.csv')\n",
      "http://en.wikipedia.org/wiki/Tere_Liye_(film): HTTPError = 404\n",
      "http://en.wikipedia.org/wiki/Pandavar_Bhoomi: URLError = [Errno 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\n",
      "http://en.wikipedia.org/wiki/Over-Thirty_Alumnus_Association: HTTPError = 404\n",
      "('1212', ' records for the year ', '2001', 'written to ', 'wikipedia_plots_2001.csv')\n",
      "http://en.wikipedia.org/wiki/Make-Up_(2002_film): HTTPError = 404\n",
      "('1257', ' records for the year ', '2002', 'written to ', 'wikipedia_plots_2002.csv')\n",
      "http://en.wikipedia.org/wiki/Vijayam: HTTPError = 404\n",
      "http://en.wikipedia.org/wiki/Lloyd_2:_Growing_Up: HTTPError = 404\n",
      "http://en.wikipedia.org/wiki/Ravedactyl:_Project_Evolution: HTTPError = 404\n",
      "http://en.wikipedia.org/wiki/Penetration_Angst: HTTPError = 404\n",
      "http://en.wikipedia.org/wiki/MTV_Movie_Awards_Reloaded: HTTPError = 404\n",
      "('1374', ' records for the year ', '2003', 'written to ', 'wikipedia_plots_2003.csv')\n",
      "http://en.wikipedia.org/wiki/A_Beautiful_Mind..._of_a_Gladiator: HTTPError = 404\n",
      "http://en.wikipedia.org/wiki/Expendable_(film): HTTPError = 404\n",
      "http://en.wikipedia.org/wiki/Smile_Please_(film): HTTPError = 404\n",
      "('1499', ' records for the year ', '2004', 'written to ', 'wikipedia_plots_2004.csv')\n",
      "http://en.wikipedia.org/wiki/Across_the_Hall_(2005_film): HTTPError = 404\n",
      "http://en.wikipedia.org/wiki/Forest_Chainsaw_Massacre: HTTPError = 404\n",
      "http://en.wikipedia.org/wiki/The_Death_of_Seasons: HTTPError = 404\n",
      "('1622', ' records for the year ', '2005', 'written to ', 'wikipedia_plots_2005.csv')\n",
      "http://en.wikipedia.org/wiki/Bekhal's_Tears: HTTPError = 404"
     ]
    }
   ],
   "source": [
    "for year in years:\n",
    "    dbp_df = get_dbpedia_data_for_year(year)\n",
    "    dbp_df['wiki_plot'] = dbp_df.wikipediaLink.apply(lambda link:get_wikipedia_plot(link))\n",
    "    csv_filename = \"wikipedia_plots_\" + str(year) + \".csv\"\n",
    "    dbp_df.to_csv(csv_filename, sep='\\t', encoding='utf-8')\n",
    "    print(str(len(dbp_df)), \" records for the year \", str(year), \"written to \", csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
